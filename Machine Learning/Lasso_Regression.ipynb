{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {
        "id": "3kiPV0IvR_df"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import \\\n",
        "    r2_score, get_scorer\n",
        "from sklearn.linear_model import \\\n",
        "    Lasso, Ridge, LassoCV,LinearRegression\n",
        "from sklearn.preprocessing import \\\n",
        "    StandardScaler, PolynomialFeatures\n",
        "from sklearn.model_selection import \\\n",
        "    KFold, RepeatedKFold, GridSearchCV, \\\n",
        "    cross_validate, train_test_split    \n",
        "from sklearn import decomposition, datasets\n",
        "from sklearn import linear_model\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv (\"/content/heart.csv\")\n",
        "print(df.columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Do1EAEfSSJMm",
        "outputId": "274a296f-e413-4b24-c608-fb6f9f991669"
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach',\n",
            "       'exang', 'oldpeak', 'slope', 'ca', 'thal', 'target'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "cmap =sns.diverging_palette(5, 250, as_cmap=True)\n",
        "def magnify():\n",
        "    return [dict(selector=\"th\",\n",
        "                 props=[(\"font-size\", \"7pt\")]),\n",
        "            dict(selector=\"td\",\n",
        "                 props=[('padding', \"0em 0em\")]),\n",
        "            dict(selector=\"th:hover\",\n",
        "                 props=[(\"font-size\", \"12pt\")]),\n",
        "            dict(selector=\"tr:hover td:hover\",\n",
        "                 props=[('max-width', '200px'),\n",
        "                        ('font-size', '12pt')])\n",
        "]\n",
        "df.corr().style.background_gradient(cmap, axis=1)\\\n",
        "    .set_properties(**{'max-width': '80px', 'font-size': '10pt'})\\\n",
        "    .set_caption(\"Hover to magify\")\\\n",
        "    .set_precision(2)\\\n",
        "    .set_table_styles(magnify())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        },
        "id": "m5kmkP2NSJXC",
        "outputId": "8232d9eb-3e4a-481b-b9af-5f4fc43bd2bf"
      },
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-131-2898e8aeae7d>:14: FutureWarning: this method is deprecated in favour of `Styler.format(precision=..)`\n",
            "  df.corr().style.background_gradient(cmap, axis=1)\\\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7efd52838ca0>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "#T_87765_ th {\n",
              "  font-size: 7pt;\n",
              "}\n",
              "#T_87765_ td {\n",
              "  padding: 0em 0em;\n",
              "}\n",
              "#T_87765_ th:hover {\n",
              "  font-size: 12pt;\n",
              "}\n",
              "#T_87765_ tr:hover td:hover {\n",
              "  max-width: 200px;\n",
              "  font-size: 12pt;\n",
              "}\n",
              "#T_87765_row0_col0, #T_87765_row1_col1, #T_87765_row2_col2, #T_87765_row3_col3, #T_87765_row4_col4, #T_87765_row5_col5, #T_87765_row6_col6, #T_87765_row7_col7, #T_87765_row8_col8, #T_87765_row9_col9, #T_87765_row10_col10, #T_87765_row11_col11, #T_87765_row12_col12, #T_87765_row13_col13 {\n",
              "  background-color: #4479bb;\n",
              "  color: #f1f1f1;\n",
              "  max-width: 80px;\n",
              "  font-size: 10pt;\n",
              "}\n",
              "#T_87765_row0_col1, #T_87765_row5_col0, #T_87765_row6_col10 {\n",
              "  background-color: #e28698;\n",
              "  color: #f1f1f1;\n",
              "  max-width: 80px;\n",
              "  font-size: 10pt;\n",
              "}\n",
              "#T_87765_row0_col2, #T_87765_row2_col4, #T_87765_row4_col11 {\n",
              "  background-color: #e48e9f;\n",
              "  color: #f1f1f1;\n",
              "  max-width: 80px;\n",
              "  font-size: 10pt;\n",
              "}\n",
              "#T_87765_row0_col3, #T_87765_row0_col11, #T_87765_row11_col0 {\n",
              "  background-color: #f1e8ea;\n",
              "  color: #000000;\n",
              "  max-width: 80px;\n",
              "  font-size: 10pt;\n",
              "}\n",
              "#T_87765_row0_col4 {\n",
              "  background-color: #efdcdf;\n",
              "  color: #000000;\n",
              "  max-width: 80px;\n",
              "  font-size: 10pt;\n",
              "}\n",
              "#T_87765_row0_col5 {\n",
              "  background-color: #ebc2ca;\n",
              "  color: #000000;\n",
              "  max-width: 80px;\n",
              "  font-size: 10pt;\n",
              "}\n",
              "#T_87765_row0_col6, #T_87765_row1_col2 {\n",
              "  background-color: #e17e92;\n",
              "  color: #f1f1f1;\n",
              "  max-width: 80px;\n",
              "  font-size: 10pt;\n",
              "}\n",
              "#T_87765_row0_col7, #T_87765_row1_col13, #T_87765_row2_col8, #T_87765_row3_col13, #T_87765_row4_col1, #T_87765_row5_col6, #T_87765_row6_col4, #T_87765_row7_col0, #T_87765_row8_col13, #T_87765_row9_col10, #T_87765_row10_col9, #T_87765_row11_col13, #T_87765_row12_col13, #T_87765_row13_col8, #T_87765_row13_col9 {\n",
              "  background-color: #d73c5b;\n",
              "  color: #f1f1f1;\n",
              "  max-width: 80px;\n",
              "  font-size: 10pt;\n",
              "}\n",
              "#T_87765_row0_col8, #T_87765_row8_col3 {\n",
              "  background-color: #eab9c3;\n",
              "  color: #000000;\n",
              "  max-width: 80px;\n",
              "  font-size: 10pt;\n",
              "}\n",
              "#T_87765_row0_col9 {\n",
              "  background-color: #efd9dd;\n",
              "  color: #000000;\n",
              "  max-width: 80px;\n",
              "  font-size: 10pt;\n",
              "}\n",
              "#T_87765_row0_col10, #T_87765_row1_col3 {\n",
              "  background-color: #e0748a;\n",
              "  color: #f1f1f1;\n",
              "  max-width: 80px;\n",
              "  font-size: 10pt;\n",
              "}\n",
              "#T_87765_row0_col12, #T_87765_row9_col6 {\n",
              "  background-color: #e9b5bf;\n",
              "  color: #000000;\n",
              "  max-width: 80px;\n",
              "  font-size: 10pt;\n",
              "}\n",
              "#T_87765_row0_col13 {\n",
              "  background-color: #dd657d;\n",
              "  color: #f1f1f1;\n",
              "  max-width: 80px;\n",
              "  font-size: 10pt;\n",
              "}\n",
              "#T_87765_row1_col0, #T_87765_row5_col8 {\n",
              "  background-color: #df6d84;\n",
              "  color: #f1f1f1;\n",
              "  max-width: 80px;\n",
              "  font-size: 10pt;\n",
              "}\n",
              "#T_87765_row1_col4 {\n",
              "  background-color: #db526e;\n",
              "  color: #f1f1f1;\n",
              "  max-width: 80px;\n",
              "  font-size: 10pt;\n",
              "}\n",
              "#T_87765_row1_col5 {\n",
              "  background-color: #e492a3;\n",
              "  color: #f1f1f1;\n",
              "  max-width: 80px;\n",
              "  font-size: 10pt;\n",
              "}\n",
              "#T_87765_row1_col6, #T_87765_row3_col8, #T_87765_row3_col12 {\n",
              "  background-color: #e17a8f;\n",
              "  color: #f1f1f1;\n",
              "  max-width: 80px;\n",
              "  font-size: 10pt;\n",
              "}\n",
              "#T_87765_row1_col7, #T_87765_row12_col10 {\n",
              "  background-color: #e17d91;\n",
              "  color: #f1f1f1;\n",
              "  max-width: 80px;\n",
              "  font-size: 10pt;\n",
              "}\n",
              "#T_87765_row1_col8, #T_87765_row10_col5, #T_87765_row12_col4 {\n",
              "  background-color: #e9b2bd;\n",
              "  color: #000000;\n",
              "  max-width: 80px;\n",
              "  font-size: 10pt;\n",
              "}\n",
              "#T_87765_row1_col9, #T_87765_row3_col5 {\n",
              "  background-color: #e7a3b0;\n",
              "  color: #000000;\n",
              "  max-width: 80px;\n",
              "  font-size: 10pt;\n",
              "}\n",
              "#T_87765_row1_col10, #T_87765_row10_col8 {\n",
              "  background-color: #e28396;\n",
              "  color: #f1f1f1;\n",
              "  max-width: 80px;\n",
              "  font-size: 10pt;\n",
              "}\n",
              "#T_87765_row1_col11, #T_87765_row10_col12, #T_87765_row12_col0 {\n",
              "  background-color: #e8abb7;\n",
              "  color: #000000;\n",
              "  max-width: 80px;\n",
              "  font-size: 10pt;\n",
              "}\n",
              "#T_87765_row1_col12, #T_87765_row9_col5 {\n",
              "  background-color: #ecc3cb;\n",
              "  color: #000000;\n",
              "  max-width: 80px;\n",
              "  font-size: 10pt;\n",
              "}\n",
              "#T_87765_row2_col0, #T_87765_row12_col6, #T_87765_row13_col4 {\n",
              "  background-color: #e491a1;\n",
              "  color: #f1f1f1;\n",
              "  max-width: 80px;\n",
              "  font-size: 10pt;\n",
              "}\n",
              "#T_87765_row2_col1, #T_87765_row9_col2, #T_87765_row10_col0 {\n",
              "  background-color: #e599a8;\n",
              "  color: #000000;\n",
              "  max-width: 80px;\n",
              "  font-size: 10pt;\n",
              "}\n",
              "#T_87765_row2_col3, #T_87765_row7_col6 {\n",
              "  background-color: #e8aeba;\n",
              "  color: #000000;\n",
              "  max-width: 80px;\n",
              "  font-size: 10pt;\n",
              "}\n",
              "#T_87765_row2_col5 {\n",
              "  background-color: #eab8c2;\n",
              "  color: #000000;\n",
              "  max-width: 80px;\n",
              "  font-size: 10pt;\n",
              "}\n",
              "#T_87765_row2_col6, #T_87765_row10_col11 {\n",
              "  background-color: #e9afbb;\n",
              "  color: #000000;\n",
              "  max-width: 80px;\n",
              "  font-size: 10pt;\n",
              "}\n",
              "#T_87765_row2_col7, #T_87765_row9_col11 {\n",
              "  background-color: #eff0f2;\n",
              "  color: #000000;\n",
              "  max-width: 80px;\n",
              "  font-size: 10pt;\n",
              "}\n",
              "#T_87765_row2_col9, #T_87765_row2_col11 {\n",
              "  background-color: #e0768b;\n",
              "  color: #f1f1f1;\n",
              "  max-width: 80px;\n",
              "  font-size: 10pt;\n",
              "}\n",
              "#T_87765_row2_col10, #T_87765_row8_col11 {\n",
              "  background-color: #ecc6ce;\n",
              "  color: #000000;\n",
              "  max-width: 80px;\n",
              "  font-size: 10pt;\n",
              "}\n",
              "#T_87765_row2_col12, #T_87765_row6_col7 {\n",
              "  background-color: #e0798d;\n",
              "  color: #f1f1f1;\n",
              "  max-width: 80px;\n",
              "  font-size: 10pt;\n",
              "}\n",
              "#T_87765_row2_col13 {\n",
              "  background-color: #d0dae8;\n",
              "  color: #000000;\n",
              "  max-width: 80px;\n",
              "  font-size: 10pt;\n",
              "}\n",
              "#T_87765_row3_col0 {\n",
              "  background-color: #ebbfc8;\n",
              "  color: #000000;\n",
              "  max-width: 80px;\n",
              "  font-size: 10pt;\n",
              "}\n",
              "#T_87765_row3_col1 {\n",
              "  background-color: #da4e6a;\n",
              "  color: #f1f1f1;\n",
              "  max-width: 80px;\n",
              "  font-size: 10pt;\n",
              "}\n",
              "#T_87765_row3_col2, #T_87765_row4_col10 {\n",
              "  background-color: #e07389;\n",
              "  color: #f1f1f1;\n",
              "  max-width: 80px;\n",
              "  font-size: 10pt;\n",
              "}\n",
              "#T_87765_row3_col4 {\n",
              "  background-color: #e48fa0;\n",
              "  color: #f1f1f1;\n",
              "  max-width: 80px;\n",
              "  font-size: 10pt;\n",
              "}\n",
              "#T_87765_row3_col6, #T_87765_row6_col0 {\n",
              "  background-color: #d8405e;\n",
              "  color: #f1f1f1;\n",
              "  max-width: 80px;\n",
              "  font-size: 10pt;\n",
              "}\n",
              "#T_87765_row3_col7, #T_87765_row5_col7, #T_87765_row9_col13 {\n",
              "  background-color: #dc5b75;\n",
              "  color: #f1f1f1;\n",
              "  max-width: 80px;\n",
              "  font-size: 10pt;\n",
              "}\n",
              "#T_87765_row3_col9, #T_87765_row10_col3 {\n",
              "  background-color: #e7a4b1;\n",
              "  color: #000000;\n",
              "  max-width: 80px;\n",
              "  font-size: 10pt;\n",
              "}\n",
              "#T_87765_row3_col10 {\n",
              "  background-color: #d8415f;\n",
              "  color: #f1f1f1;\n",
              "  max-width: 80px;\n",
              "  font-size: 10pt;\n",
              "}\n",
              "#T_87765_row3_col11 {\n",
              "  background-color: #e3889a;\n",
              "  color: #f1f1f1;\n",
              "  max-width: 80px;\n",
              "  font-size: 10pt;\n",
              "}\n",
              "#T_87765_row4_col0, #T_87765_row8_col4, #T_87765_row10_col1 {\n",
              "  background-color: #eabbc4;\n",
              "  color: #000000;\n",
              "  max-width: 80px;\n",
              "  font-size: 10pt;\n",
              "}\n",
              "#T_87765_row4_col2 {\n",
              "  background-color: #dc5e77;\n",
              "  color: #f1f1f1;\n",
              "  max-width: 80px;\n",
              "  font-size: 10pt;\n",
              "}\n",
              "#T_87765_row4_col3 {\n",
              "  background-color: #e69ead;\n",
              "  color: #000000;\n",
              "  max-width: 80px;\n",
              "  font-size: 10pt;\n",
              "}\n",
              "#T_87765_row4_col5 {\n",
              "  background-color: #e18093;\n",
              "  color: #f1f1f1;\n",
              "  max-width: 80px;\n",
              "  font-size: 10pt;\n",
              "}\n",
              "#T_87765_row4_col6, #T_87765_row8_col7, #T_87765_row13_col11 {\n",
              "  background-color: #d94a67;\n",
              "  color: #f1f1f1;\n",
              "  max-width: 80px;\n",
              "  font-size: 10pt;\n",
              "}\n",
              "#T_87765_row4_col7, #T_87765_row13_col0 {\n",
              "  background-color: #df7086;\n",
              "  color: #f1f1f1;\n",
              "  max-width: 80px;\n",
              "  font-size: 10pt;\n",
              "}\n",
              "#T_87765_row4_col8, #T_87765_row4_col9, #T_87765_row11_col6, #T_87765_row12_col5 {\n",
              "  background-color: #e38b9d;\n",
              "  color: #f1f1f1;\n",
              "  max-width: 80px;\n",
              "  font-size: 10pt;\n",
              "}\n",
              "#T_87765_row4_col12 {\n",
              "  background-color: #e595a5;\n",
              "  color: #000000;\n",
              "  max-width: 80px;\n",
              "  font-size: 10pt;\n",
              "}\n",
              "#T_87765_row4_col13, #T_87765_row6_col1 {\n",
              "  background-color: #db5872;\n",
              "  color: #f1f1f1;\n",
              "  max-width: 80px;\n",
              "  font-size: 10pt;\n",
              "}\n",
              "#T_87765_row5_col1, #T_87765_row5_col4, #T_87765_row8_col10 {\n",
              "  background-color: #de667e;\n",
              "  color: #f1f1f1;\n",
              "  max-width: 80px;\n",
              "  font-size: 10pt;\n",
              "}\n",
              "#T_87765_row5_col2, #T_87765_row6_col2 {\n",
              "  background-color: #e0778c;\n",
              "  color: #f1f1f1;\n",
              "  max-width: 80px;\n",
              "  font-size: 10pt;\n",
              "}\n",
              "#T_87765_row5_col3, #T_87765_row8_col6 {\n",
              "  background-color: #e59aa9;\n",
              "  color: #000000;\n",
              "  max-width: 80px;\n",
              "  font-size: 10pt;\n",
              "}\n",
              "#T_87765_row5_col9 {\n",
              "  background-color: #dd6179;\n",
              "  color: #f1f1f1;\n",
              "  max-width: 80px;\n",
              "  font-size: 10pt;\n",
              "}\n",
              "#T_87765_row5_col10, #T_87765_row6_col5 {\n",
              "  background-color: #d94865;\n",
              "  color: #f1f1f1;\n",
              "  max-width: 80px;\n",
              "  font-size: 10pt;\n",
              "}\n",
              "#T_87765_row5_col11 {\n",
              "  background-color: #e38a9c;\n",
              "  color: #f1f1f1;\n",
              "  max-width: 80px;\n",
              "  font-size: 10pt;\n",
              "}\n",
              "#T_87765_row5_col12, #T_87765_row5_col13 {\n",
              "  background-color: #da506b;\n",
              "  color: #f1f1f1;\n",
              "  max-width: 80px;\n",
              "  font-size: 10pt;\n",
              "}\n",
              "#T_87765_row6_col3 {\n",
              "  background-color: #d84361;\n",
              "  color: #f1f1f1;\n",
              "  max-width: 80px;\n",
              "  font-size: 10pt;\n",
              "}\n",
              "#T_87765_row6_col8 {\n",
              "  background-color: #db5570;\n",
              "  color: #f1f1f1;\n",
              "  max-width: 80px;\n",
              "  font-size: 10pt;\n",
              "}\n",
              "#T_87765_row6_col9 {\n",
              "  background-color: #dc5973;\n",
              "  color: #f1f1f1;\n",
              "  max-width: 80px;\n",
              "  font-size: 10pt;\n",
              "}\n",
              "#T_87765_row6_col11 {\n",
              "  background-color: #da516c;\n",
              "  color: #f1f1f1;\n",
              "  max-width: 80px;\n",
              "  font-size: 10pt;\n",
              "}\n",
              "#T_87765_row6_col12, #T_87765_row13_col1 {\n",
              "  background-color: #dd637c;\n",
              "  color: #f1f1f1;\n",
              "  max-width: 80px;\n",
              "  font-size: 10pt;\n",
              "}\n",
              "#T_87765_row6_col13, #T_87765_row7_col1 {\n",
              "  background-color: #e494a4;\n",
              "  color: #f1f1f1;\n",
              "  max-width: 80px;\n",
              "  font-size: 10pt;\n",
              "}\n",
              "#T_87765_row7_col2 {\n",
              "  background-color: #f1f1f2;\n",
              "  color: #000000;\n",
              "  max-width: 80px;\n",
              "  font-size: 10pt;\n",
              "}\n",
              "#T_87765_row7_col3 {\n",
              "  background-color: #e597a7;\n",
              "  color: #000000;\n",
              "  max-width: 80px;\n",
              "  font-size: 10pt;\n",
              "}\n",
              "#T_87765_row7_col4 {\n",
              "  background-color: #e69caa;\n",
              "  color: #000000;\n",
              "  max-width: 80px;\n",
              "  font-size: 10pt;\n",
              "}\n",
              "#T_87765_row7_col5, #T_87765_row13_col5 {\n",
              "  background-color: #e6a0ae;\n",
              "  color: #000000;\n",
              "  max-width: 80px;\n",
              "  font-size: 10pt;\n",
              "}\n",
              "#T_87765_row7_col8 {\n",
              "  background-color: #d73d5c;\n",
              "  color: #f1f1f1;\n",
              "  max-width: 80px;\n",
              "  font-size: 10pt;\n",
              "}\n",
              "#T_87765_row7_col9 {\n",
              "  background-color: #d94663;\n",
              "  color: #f1f1f1;\n",
              "  max-width: 80px;\n",
              "  font-size: 10pt;\n",
              "}\n",
              "#T_87765_row7_col10 {\n",
              "  background-color: #dbe2eb;\n",
              "  color: #000000;\n",
              "  max-width: 80px;\n",
              "  font-size: 10pt;\n",
              "}\n",
              "#T_87765_row7_col11, #T_87765_row12_col2 {\n",
              "  background-color: #de6b82;\n",
              "  color: #f1f1f1;\n",
              "  max-width: 80px;\n",
              "  font-size: 10pt;\n",
              "}\n",
              "#T_87765_row7_col12, #T_87765_row13_col3 {\n",
              "  background-color: #e28799;\n",
              "  color: #f1f1f1;\n",
              "  max-width: 80px;\n",
              "  font-size: 10pt;\n",
              "}\n",
              "#T_87765_row7_col13, #T_87765_row10_col13 {\n",
              "  background-color: #d4dde9;\n",
              "  color: #000000;\n",
              "  max-width: 80px;\n",
              "  font-size: 10pt;\n",
              "}\n",
              "#T_87765_row8_col0, #T_87765_row12_col11 {\n",
              "  background-color: #ebc1c9;\n",
              "  color: #000000;\n",
              "  max-width: 80px;\n",
              "  font-size: 10pt;\n",
              "}\n",
              "#T_87765_row8_col1, #T_87765_row12_col1, #T_87765_row12_col8 {\n",
              "  background-color: #edcdd3;\n",
              "  color: #000000;\n",
              "  max-width: 80px;\n",
              "  font-size: 10pt;\n",
              "}\n",
              "#T_87765_row8_col2 {\n",
              "  background-color: #d84462;\n",
              "  color: #f1f1f1;\n",
              "  max-width: 80px;\n",
              "  font-size: 10pt;\n",
              "}\n",
              "#T_87765_row8_col5 {\n",
              "  background-color: #eab7c1;\n",
              "  color: #000000;\n",
              "  max-width: 80px;\n",
              "  font-size: 10pt;\n",
              "}\n",
              "#T_87765_row8_col9 {\n",
              "  background-color: #eaecf0;\n",
              "  color: #000000;\n",
              "  max-width: 80px;\n",
              "  font-size: 10pt;\n",
              "}\n",
              "#T_87765_row8_col12 {\n",
              "  background-color: #efdde0;\n",
              "  color: #000000;\n",
              "  max-width: 80px;\n",
              "  font-size: 10pt;\n",
              "}\n",
              "#T_87765_row9_col0 {\n",
              "  background-color: #f2f1f1;\n",
              "  color: #000000;\n",
              "  max-width: 80px;\n",
              "  font-size: 10pt;\n",
              "}\n",
              "#T_87765_row9_col1, #T_87765_row10_col6 {\n",
              "  background-color: #eed4d9;\n",
              "  color: #000000;\n",
              "  max-width: 80px;\n",
              "  font-size: 10pt;\n",
              "}\n",
              "#T_87765_row9_col3 {\n",
              "  background-color: #f2ebec;\n",
              "  color: #000000;\n",
              "  max-width: 80px;\n",
              "  font-size: 10pt;\n",
              "}\n",
              "#T_87765_row9_col4 {\n",
              "  background-color: #edd0d6;\n",
              "  color: #000000;\n",
              "  max-width: 80px;\n",
              "  font-size: 10pt;\n",
              "}\n",
              "#T_87765_row9_col7 {\n",
              "  background-color: #df6f85;\n",
              "  color: #f1f1f1;\n",
              "  max-width: 80px;\n",
              "  font-size: 10pt;\n",
              "}\n",
              "#T_87765_row9_col8 {\n",
              "  background-color: #dce3ec;\n",
              "  color: #000000;\n",
              "  max-width: 80px;\n",
              "  font-size: 10pt;\n",
              "}\n",
              "#T_87765_row9_col12 {\n",
              "  background-color: #f2eff0;\n",
              "  color: #000000;\n",
              "  max-width: 80px;\n",
              "  font-size: 10pt;\n",
              "}\n",
              "#T_87765_row10_col2 {\n",
              "  background-color: #f0dee2;\n",
              "  color: #000000;\n",
              "  max-width: 80px;\n",
              "  font-size: 10pt;\n",
              "}\n",
              "#T_87765_row10_col4, #T_87765_row11_col1 {\n",
              "  background-color: #ebbec6;\n",
              "  color: #000000;\n",
              "  max-width: 80px;\n",
              "  font-size: 10pt;\n",
              "}\n",
              "#T_87765_row10_col7 {\n",
              "  background-color: #c9d6e6;\n",
              "  color: #000000;\n",
              "  max-width: 80px;\n",
              "  font-size: 10pt;\n",
              "}\n",
              "#T_87765_row11_col2 {\n",
              "  background-color: #df7288;\n",
              "  color: #f1f1f1;\n",
              "  max-width: 80px;\n",
              "  font-size: 10pt;\n",
              "}\n",
              "#T_87765_row11_col3, #T_87765_row11_col8 {\n",
              "  background-color: #eabcc5;\n",
              "  color: #000000;\n",
              "  max-width: 80px;\n",
              "  font-size: 10pt;\n",
              "}\n",
              "#T_87765_row11_col4 {\n",
              "  background-color: #e9b4be;\n",
              "  color: #000000;\n",
              "  max-width: 80px;\n",
              "  font-size: 10pt;\n",
              "}\n",
              "#T_87765_row11_col5 {\n",
              "  background-color: #ecc5cc;\n",
              "  color: #000000;\n",
              "  max-width: 80px;\n",
              "  font-size: 10pt;\n",
              "}\n",
              "#T_87765_row11_col7 {\n",
              "  background-color: #de6980;\n",
              "  color: #f1f1f1;\n",
              "  max-width: 80px;\n",
              "  font-size: 10pt;\n",
              "}\n",
              "#T_87765_row11_col9 {\n",
              "  background-color: #efdade;\n",
              "  color: #000000;\n",
              "  max-width: 80px;\n",
              "  font-size: 10pt;\n",
              "}\n",
              "#T_87765_row11_col10 {\n",
              "  background-color: #e38d9e;\n",
              "  color: #f1f1f1;\n",
              "  max-width: 80px;\n",
              "  font-size: 10pt;\n",
              "}\n",
              "#T_87765_row11_col12 {\n",
              "  background-color: #ecc8cf;\n",
              "  color: #000000;\n",
              "  max-width: 80px;\n",
              "  font-size: 10pt;\n",
              "}\n",
              "#T_87765_row12_col3 {\n",
              "  background-color: #e7a7b4;\n",
              "  color: #000000;\n",
              "  max-width: 80px;\n",
              "  font-size: 10pt;\n",
              "}\n",
              "#T_87765_row12_col7 {\n",
              "  background-color: #e17c90;\n",
              "  color: #f1f1f1;\n",
              "  max-width: 80px;\n",
              "  font-size: 10pt;\n",
              "}\n",
              "#T_87765_row12_col9 {\n",
              "  background-color: #edcfd5;\n",
              "  color: #000000;\n",
              "  max-width: 80px;\n",
              "  font-size: 10pt;\n",
              "}\n",
              "#T_87765_row13_col2 {\n",
              "  background-color: #ccd8e7;\n",
              "  color: #000000;\n",
              "  max-width: 80px;\n",
              "  font-size: 10pt;\n",
              "}\n",
              "#T_87765_row13_col6 {\n",
              "  background-color: #edccd2;\n",
              "  color: #000000;\n",
              "  max-width: 80px;\n",
              "  font-size: 10pt;\n",
              "}\n",
              "#T_87765_row13_col7 {\n",
              "  background-color: #cfdae7;\n",
              "  color: #000000;\n",
              "  max-width: 80px;\n",
              "  font-size: 10pt;\n",
              "}\n",
              "#T_87765_row13_col10 {\n",
              "  background-color: #e2e7ee;\n",
              "  color: #000000;\n",
              "  max-width: 80px;\n",
              "  font-size: 10pt;\n",
              "}\n",
              "#T_87765_row13_col12 {\n",
              "  background-color: #db546f;\n",
              "  color: #f1f1f1;\n",
              "  max-width: 80px;\n",
              "  font-size: 10pt;\n",
              "}\n",
              "</style>\n",
              "<table id=\"T_87765_\" class=\"dataframe\">\n",
              "  <caption>Hover to magify</caption>\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th class=\"col_heading level0 col0\" >age</th>\n",
              "      <th class=\"col_heading level0 col1\" >sex</th>\n",
              "      <th class=\"col_heading level0 col2\" >cp</th>\n",
              "      <th class=\"col_heading level0 col3\" >trestbps</th>\n",
              "      <th class=\"col_heading level0 col4\" >chol</th>\n",
              "      <th class=\"col_heading level0 col5\" >fbs</th>\n",
              "      <th class=\"col_heading level0 col6\" >restecg</th>\n",
              "      <th class=\"col_heading level0 col7\" >thalach</th>\n",
              "      <th class=\"col_heading level0 col8\" >exang</th>\n",
              "      <th class=\"col_heading level0 col9\" >oldpeak</th>\n",
              "      <th class=\"col_heading level0 col10\" >slope</th>\n",
              "      <th class=\"col_heading level0 col11\" >ca</th>\n",
              "      <th class=\"col_heading level0 col12\" >thal</th>\n",
              "      <th class=\"col_heading level0 col13\" >target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_87765_level0_row0\" class=\"row_heading level0 row0\" >age</th>\n",
              "      <td id=\"T_87765_row0_col0\" class=\"data row0 col0\" >1.00</td>\n",
              "      <td id=\"T_87765_row0_col1\" class=\"data row0 col1\" >-0.10</td>\n",
              "      <td id=\"T_87765_row0_col2\" class=\"data row0 col2\" >-0.07</td>\n",
              "      <td id=\"T_87765_row0_col3\" class=\"data row0 col3\" >0.27</td>\n",
              "      <td id=\"T_87765_row0_col4\" class=\"data row0 col4\" >0.22</td>\n",
              "      <td id=\"T_87765_row0_col5\" class=\"data row0 col5\" >0.12</td>\n",
              "      <td id=\"T_87765_row0_col6\" class=\"data row0 col6\" >-0.13</td>\n",
              "      <td id=\"T_87765_row0_col7\" class=\"data row0 col7\" >-0.39</td>\n",
              "      <td id=\"T_87765_row0_col8\" class=\"data row0 col8\" >0.09</td>\n",
              "      <td id=\"T_87765_row0_col9\" class=\"data row0 col9\" >0.21</td>\n",
              "      <td id=\"T_87765_row0_col10\" class=\"data row0 col10\" >-0.17</td>\n",
              "      <td id=\"T_87765_row0_col11\" class=\"data row0 col11\" >0.27</td>\n",
              "      <td id=\"T_87765_row0_col12\" class=\"data row0 col12\" >0.07</td>\n",
              "      <td id=\"T_87765_row0_col13\" class=\"data row0 col13\" >-0.23</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_87765_level0_row1\" class=\"row_heading level0 row1\" >sex</th>\n",
              "      <td id=\"T_87765_row1_col0\" class=\"data row1 col0\" >-0.10</td>\n",
              "      <td id=\"T_87765_row1_col1\" class=\"data row1 col1\" >1.00</td>\n",
              "      <td id=\"T_87765_row1_col2\" class=\"data row1 col2\" >-0.04</td>\n",
              "      <td id=\"T_87765_row1_col3\" class=\"data row1 col3\" >-0.08</td>\n",
              "      <td id=\"T_87765_row1_col4\" class=\"data row1 col4\" >-0.20</td>\n",
              "      <td id=\"T_87765_row1_col5\" class=\"data row1 col5\" >0.03</td>\n",
              "      <td id=\"T_87765_row1_col6\" class=\"data row1 col6\" >-0.06</td>\n",
              "      <td id=\"T_87765_row1_col7\" class=\"data row1 col7\" >-0.05</td>\n",
              "      <td id=\"T_87765_row1_col8\" class=\"data row1 col8\" >0.14</td>\n",
              "      <td id=\"T_87765_row1_col9\" class=\"data row1 col9\" >0.08</td>\n",
              "      <td id=\"T_87765_row1_col10\" class=\"data row1 col10\" >-0.03</td>\n",
              "      <td id=\"T_87765_row1_col11\" class=\"data row1 col11\" >0.11</td>\n",
              "      <td id=\"T_87765_row1_col12\" class=\"data row1 col12\" >0.20</td>\n",
              "      <td id=\"T_87765_row1_col13\" class=\"data row1 col13\" >-0.28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_87765_level0_row2\" class=\"row_heading level0 row2\" >cp</th>\n",
              "      <td id=\"T_87765_row2_col0\" class=\"data row2 col0\" >-0.07</td>\n",
              "      <td id=\"T_87765_row2_col1\" class=\"data row2 col1\" >-0.04</td>\n",
              "      <td id=\"T_87765_row2_col2\" class=\"data row2 col2\" >1.00</td>\n",
              "      <td id=\"T_87765_row2_col3\" class=\"data row2 col3\" >0.04</td>\n",
              "      <td id=\"T_87765_row2_col4\" class=\"data row2 col4\" >-0.08</td>\n",
              "      <td id=\"T_87765_row2_col5\" class=\"data row2 col5\" >0.08</td>\n",
              "      <td id=\"T_87765_row2_col6\" class=\"data row2 col6\" >0.04</td>\n",
              "      <td id=\"T_87765_row2_col7\" class=\"data row2 col7\" >0.31</td>\n",
              "      <td id=\"T_87765_row2_col8\" class=\"data row2 col8\" >-0.40</td>\n",
              "      <td id=\"T_87765_row2_col9\" class=\"data row2 col9\" >-0.17</td>\n",
              "      <td id=\"T_87765_row2_col10\" class=\"data row2 col10\" >0.13</td>\n",
              "      <td id=\"T_87765_row2_col11\" class=\"data row2 col11\" >-0.18</td>\n",
              "      <td id=\"T_87765_row2_col12\" class=\"data row2 col12\" >-0.16</td>\n",
              "      <td id=\"T_87765_row2_col13\" class=\"data row2 col13\" >0.43</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_87765_level0_row3\" class=\"row_heading level0 row3\" >trestbps</th>\n",
              "      <td id=\"T_87765_row3_col0\" class=\"data row3 col0\" >0.27</td>\n",
              "      <td id=\"T_87765_row3_col1\" class=\"data row3 col1\" >-0.08</td>\n",
              "      <td id=\"T_87765_row3_col2\" class=\"data row3 col2\" >0.04</td>\n",
              "      <td id=\"T_87765_row3_col3\" class=\"data row3 col3\" >1.00</td>\n",
              "      <td id=\"T_87765_row3_col4\" class=\"data row3 col4\" >0.13</td>\n",
              "      <td id=\"T_87765_row3_col5\" class=\"data row3 col5\" >0.18</td>\n",
              "      <td id=\"T_87765_row3_col6\" class=\"data row3 col6\" >-0.12</td>\n",
              "      <td id=\"T_87765_row3_col7\" class=\"data row3 col7\" >-0.04</td>\n",
              "      <td id=\"T_87765_row3_col8\" class=\"data row3 col8\" >0.06</td>\n",
              "      <td id=\"T_87765_row3_col9\" class=\"data row3 col9\" >0.19</td>\n",
              "      <td id=\"T_87765_row3_col10\" class=\"data row3 col10\" >-0.12</td>\n",
              "      <td id=\"T_87765_row3_col11\" class=\"data row3 col11\" >0.10</td>\n",
              "      <td id=\"T_87765_row3_col12\" class=\"data row3 col12\" >0.06</td>\n",
              "      <td id=\"T_87765_row3_col13\" class=\"data row3 col13\" >-0.14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_87765_level0_row4\" class=\"row_heading level0 row4\" >chol</th>\n",
              "      <td id=\"T_87765_row4_col0\" class=\"data row4 col0\" >0.22</td>\n",
              "      <td id=\"T_87765_row4_col1\" class=\"data row4 col1\" >-0.20</td>\n",
              "      <td id=\"T_87765_row4_col2\" class=\"data row4 col2\" >-0.08</td>\n",
              "      <td id=\"T_87765_row4_col3\" class=\"data row4 col3\" >0.13</td>\n",
              "      <td id=\"T_87765_row4_col4\" class=\"data row4 col4\" >1.00</td>\n",
              "      <td id=\"T_87765_row4_col5\" class=\"data row4 col5\" >0.03</td>\n",
              "      <td id=\"T_87765_row4_col6\" class=\"data row4 col6\" >-0.15</td>\n",
              "      <td id=\"T_87765_row4_col7\" class=\"data row4 col7\" >-0.02</td>\n",
              "      <td id=\"T_87765_row4_col8\" class=\"data row4 col8\" >0.07</td>\n",
              "      <td id=\"T_87765_row4_col9\" class=\"data row4 col9\" >0.06</td>\n",
              "      <td id=\"T_87765_row4_col10\" class=\"data row4 col10\" >-0.01</td>\n",
              "      <td id=\"T_87765_row4_col11\" class=\"data row4 col11\" >0.07</td>\n",
              "      <td id=\"T_87765_row4_col12\" class=\"data row4 col12\" >0.10</td>\n",
              "      <td id=\"T_87765_row4_col13\" class=\"data row4 col13\" >-0.10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_87765_level0_row5\" class=\"row_heading level0 row5\" >fbs</th>\n",
              "      <td id=\"T_87765_row5_col0\" class=\"data row5 col0\" >0.12</td>\n",
              "      <td id=\"T_87765_row5_col1\" class=\"data row5 col1\" >0.03</td>\n",
              "      <td id=\"T_87765_row5_col2\" class=\"data row5 col2\" >0.08</td>\n",
              "      <td id=\"T_87765_row5_col3\" class=\"data row5 col3\" >0.18</td>\n",
              "      <td id=\"T_87765_row5_col4\" class=\"data row5 col4\" >0.03</td>\n",
              "      <td id=\"T_87765_row5_col5\" class=\"data row5 col5\" >1.00</td>\n",
              "      <td id=\"T_87765_row5_col6\" class=\"data row5 col6\" >-0.10</td>\n",
              "      <td id=\"T_87765_row5_col7\" class=\"data row5 col7\" >-0.01</td>\n",
              "      <td id=\"T_87765_row5_col8\" class=\"data row5 col8\" >0.05</td>\n",
              "      <td id=\"T_87765_row5_col9\" class=\"data row5 col9\" >0.01</td>\n",
              "      <td id=\"T_87765_row5_col10\" class=\"data row5 col10\" >-0.06</td>\n",
              "      <td id=\"T_87765_row5_col11\" class=\"data row5 col11\" >0.14</td>\n",
              "      <td id=\"T_87765_row5_col12\" class=\"data row5 col12\" >-0.04</td>\n",
              "      <td id=\"T_87765_row5_col13\" class=\"data row5 col13\" >-0.04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_87765_level0_row6\" class=\"row_heading level0 row6\" >restecg</th>\n",
              "      <td id=\"T_87765_row6_col0\" class=\"data row6 col0\" >-0.13</td>\n",
              "      <td id=\"T_87765_row6_col1\" class=\"data row6 col1\" >-0.06</td>\n",
              "      <td id=\"T_87765_row6_col2\" class=\"data row6 col2\" >0.04</td>\n",
              "      <td id=\"T_87765_row6_col3\" class=\"data row6 col3\" >-0.12</td>\n",
              "      <td id=\"T_87765_row6_col4\" class=\"data row6 col4\" >-0.15</td>\n",
              "      <td id=\"T_87765_row6_col5\" class=\"data row6 col5\" >-0.10</td>\n",
              "      <td id=\"T_87765_row6_col6\" class=\"data row6 col6\" >1.00</td>\n",
              "      <td id=\"T_87765_row6_col7\" class=\"data row6 col7\" >0.05</td>\n",
              "      <td id=\"T_87765_row6_col8\" class=\"data row6 col8\" >-0.07</td>\n",
              "      <td id=\"T_87765_row6_col9\" class=\"data row6 col9\" >-0.05</td>\n",
              "      <td id=\"T_87765_row6_col10\" class=\"data row6 col10\" >0.09</td>\n",
              "      <td id=\"T_87765_row6_col11\" class=\"data row6 col11\" >-0.08</td>\n",
              "      <td id=\"T_87765_row6_col12\" class=\"data row6 col12\" >-0.02</td>\n",
              "      <td id=\"T_87765_row6_col13\" class=\"data row6 col13\" >0.13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_87765_level0_row7\" class=\"row_heading level0 row7\" >thalach</th>\n",
              "      <td id=\"T_87765_row7_col0\" class=\"data row7 col0\" >-0.39</td>\n",
              "      <td id=\"T_87765_row7_col1\" class=\"data row7 col1\" >-0.05</td>\n",
              "      <td id=\"T_87765_row7_col2\" class=\"data row7 col2\" >0.31</td>\n",
              "      <td id=\"T_87765_row7_col3\" class=\"data row7 col3\" >-0.04</td>\n",
              "      <td id=\"T_87765_row7_col4\" class=\"data row7 col4\" >-0.02</td>\n",
              "      <td id=\"T_87765_row7_col5\" class=\"data row7 col5\" >-0.01</td>\n",
              "      <td id=\"T_87765_row7_col6\" class=\"data row7 col6\" >0.05</td>\n",
              "      <td id=\"T_87765_row7_col7\" class=\"data row7 col7\" >1.00</td>\n",
              "      <td id=\"T_87765_row7_col8\" class=\"data row7 col8\" >-0.38</td>\n",
              "      <td id=\"T_87765_row7_col9\" class=\"data row7 col9\" >-0.35</td>\n",
              "      <td id=\"T_87765_row7_col10\" class=\"data row7 col10\" >0.40</td>\n",
              "      <td id=\"T_87765_row7_col11\" class=\"data row7 col11\" >-0.21</td>\n",
              "      <td id=\"T_87765_row7_col12\" class=\"data row7 col12\" >-0.10</td>\n",
              "      <td id=\"T_87765_row7_col13\" class=\"data row7 col13\" >0.42</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_87765_level0_row8\" class=\"row_heading level0 row8\" >exang</th>\n",
              "      <td id=\"T_87765_row8_col0\" class=\"data row8 col0\" >0.09</td>\n",
              "      <td id=\"T_87765_row8_col1\" class=\"data row8 col1\" >0.14</td>\n",
              "      <td id=\"T_87765_row8_col2\" class=\"data row8 col2\" >-0.40</td>\n",
              "      <td id=\"T_87765_row8_col3\" class=\"data row8 col3\" >0.06</td>\n",
              "      <td id=\"T_87765_row8_col4\" class=\"data row8 col4\" >0.07</td>\n",
              "      <td id=\"T_87765_row8_col5\" class=\"data row8 col5\" >0.05</td>\n",
              "      <td id=\"T_87765_row8_col6\" class=\"data row8 col6\" >-0.07</td>\n",
              "      <td id=\"T_87765_row8_col7\" class=\"data row8 col7\" >-0.38</td>\n",
              "      <td id=\"T_87765_row8_col8\" class=\"data row8 col8\" >1.00</td>\n",
              "      <td id=\"T_87765_row8_col9\" class=\"data row8 col9\" >0.31</td>\n",
              "      <td id=\"T_87765_row8_col10\" class=\"data row8 col10\" >-0.27</td>\n",
              "      <td id=\"T_87765_row8_col11\" class=\"data row8 col11\" >0.11</td>\n",
              "      <td id=\"T_87765_row8_col12\" class=\"data row8 col12\" >0.20</td>\n",
              "      <td id=\"T_87765_row8_col13\" class=\"data row8 col13\" >-0.44</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_87765_level0_row9\" class=\"row_heading level0 row9\" >oldpeak</th>\n",
              "      <td id=\"T_87765_row9_col0\" class=\"data row9 col0\" >0.21</td>\n",
              "      <td id=\"T_87765_row9_col1\" class=\"data row9 col1\" >0.08</td>\n",
              "      <td id=\"T_87765_row9_col2\" class=\"data row9 col2\" >-0.17</td>\n",
              "      <td id=\"T_87765_row9_col3\" class=\"data row9 col3\" >0.19</td>\n",
              "      <td id=\"T_87765_row9_col4\" class=\"data row9 col4\" >0.06</td>\n",
              "      <td id=\"T_87765_row9_col5\" class=\"data row9 col5\" >0.01</td>\n",
              "      <td id=\"T_87765_row9_col6\" class=\"data row9 col6\" >-0.05</td>\n",
              "      <td id=\"T_87765_row9_col7\" class=\"data row9 col7\" >-0.35</td>\n",
              "      <td id=\"T_87765_row9_col8\" class=\"data row9 col8\" >0.31</td>\n",
              "      <td id=\"T_87765_row9_col9\" class=\"data row9 col9\" >1.00</td>\n",
              "      <td id=\"T_87765_row9_col10\" class=\"data row9 col10\" >-0.58</td>\n",
              "      <td id=\"T_87765_row9_col11\" class=\"data row9 col11\" >0.22</td>\n",
              "      <td id=\"T_87765_row9_col12\" class=\"data row9 col12\" >0.20</td>\n",
              "      <td id=\"T_87765_row9_col13\" class=\"data row9 col13\" >-0.44</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_87765_level0_row10\" class=\"row_heading level0 row10\" >slope</th>\n",
              "      <td id=\"T_87765_row10_col0\" class=\"data row10 col0\" >-0.17</td>\n",
              "      <td id=\"T_87765_row10_col1\" class=\"data row10 col1\" >-0.03</td>\n",
              "      <td id=\"T_87765_row10_col2\" class=\"data row10 col2\" >0.13</td>\n",
              "      <td id=\"T_87765_row10_col3\" class=\"data row10 col3\" >-0.12</td>\n",
              "      <td id=\"T_87765_row10_col4\" class=\"data row10 col4\" >-0.01</td>\n",
              "      <td id=\"T_87765_row10_col5\" class=\"data row10 col5\" >-0.06</td>\n",
              "      <td id=\"T_87765_row10_col6\" class=\"data row10 col6\" >0.09</td>\n",
              "      <td id=\"T_87765_row10_col7\" class=\"data row10 col7\" >0.40</td>\n",
              "      <td id=\"T_87765_row10_col8\" class=\"data row10 col8\" >-0.27</td>\n",
              "      <td id=\"T_87765_row10_col9\" class=\"data row10 col9\" >-0.58</td>\n",
              "      <td id=\"T_87765_row10_col10\" class=\"data row10 col10\" >1.00</td>\n",
              "      <td id=\"T_87765_row10_col11\" class=\"data row10 col11\" >-0.07</td>\n",
              "      <td id=\"T_87765_row10_col12\" class=\"data row10 col12\" >-0.09</td>\n",
              "      <td id=\"T_87765_row10_col13\" class=\"data row10 col13\" >0.35</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_87765_level0_row11\" class=\"row_heading level0 row11\" >ca</th>\n",
              "      <td id=\"T_87765_row11_col0\" class=\"data row11 col0\" >0.27</td>\n",
              "      <td id=\"T_87765_row11_col1\" class=\"data row11 col1\" >0.11</td>\n",
              "      <td id=\"T_87765_row11_col2\" class=\"data row11 col2\" >-0.18</td>\n",
              "      <td id=\"T_87765_row11_col3\" class=\"data row11 col3\" >0.10</td>\n",
              "      <td id=\"T_87765_row11_col4\" class=\"data row11 col4\" >0.07</td>\n",
              "      <td id=\"T_87765_row11_col5\" class=\"data row11 col5\" >0.14</td>\n",
              "      <td id=\"T_87765_row11_col6\" class=\"data row11 col6\" >-0.08</td>\n",
              "      <td id=\"T_87765_row11_col7\" class=\"data row11 col7\" >-0.21</td>\n",
              "      <td id=\"T_87765_row11_col8\" class=\"data row11 col8\" >0.11</td>\n",
              "      <td id=\"T_87765_row11_col9\" class=\"data row11 col9\" >0.22</td>\n",
              "      <td id=\"T_87765_row11_col10\" class=\"data row11 col10\" >-0.07</td>\n",
              "      <td id=\"T_87765_row11_col11\" class=\"data row11 col11\" >1.00</td>\n",
              "      <td id=\"T_87765_row11_col12\" class=\"data row11 col12\" >0.15</td>\n",
              "      <td id=\"T_87765_row11_col13\" class=\"data row11 col13\" >-0.38</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_87765_level0_row12\" class=\"row_heading level0 row12\" >thal</th>\n",
              "      <td id=\"T_87765_row12_col0\" class=\"data row12 col0\" >0.07</td>\n",
              "      <td id=\"T_87765_row12_col1\" class=\"data row12 col1\" >0.20</td>\n",
              "      <td id=\"T_87765_row12_col2\" class=\"data row12 col2\" >-0.16</td>\n",
              "      <td id=\"T_87765_row12_col3\" class=\"data row12 col3\" >0.06</td>\n",
              "      <td id=\"T_87765_row12_col4\" class=\"data row12 col4\" >0.10</td>\n",
              "      <td id=\"T_87765_row12_col5\" class=\"data row12 col5\" >-0.04</td>\n",
              "      <td id=\"T_87765_row12_col6\" class=\"data row12 col6\" >-0.02</td>\n",
              "      <td id=\"T_87765_row12_col7\" class=\"data row12 col7\" >-0.10</td>\n",
              "      <td id=\"T_87765_row12_col8\" class=\"data row12 col8\" >0.20</td>\n",
              "      <td id=\"T_87765_row12_col9\" class=\"data row12 col9\" >0.20</td>\n",
              "      <td id=\"T_87765_row12_col10\" class=\"data row12 col10\" >-0.09</td>\n",
              "      <td id=\"T_87765_row12_col11\" class=\"data row12 col11\" >0.15</td>\n",
              "      <td id=\"T_87765_row12_col12\" class=\"data row12 col12\" >1.00</td>\n",
              "      <td id=\"T_87765_row12_col13\" class=\"data row12 col13\" >-0.34</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_87765_level0_row13\" class=\"row_heading level0 row13\" >target</th>\n",
              "      <td id=\"T_87765_row13_col0\" class=\"data row13 col0\" >-0.23</td>\n",
              "      <td id=\"T_87765_row13_col1\" class=\"data row13 col1\" >-0.28</td>\n",
              "      <td id=\"T_87765_row13_col2\" class=\"data row13 col2\" >0.43</td>\n",
              "      <td id=\"T_87765_row13_col3\" class=\"data row13 col3\" >-0.14</td>\n",
              "      <td id=\"T_87765_row13_col4\" class=\"data row13 col4\" >-0.10</td>\n",
              "      <td id=\"T_87765_row13_col5\" class=\"data row13 col5\" >-0.04</td>\n",
              "      <td id=\"T_87765_row13_col6\" class=\"data row13 col6\" >0.13</td>\n",
              "      <td id=\"T_87765_row13_col7\" class=\"data row13 col7\" >0.42</td>\n",
              "      <td id=\"T_87765_row13_col8\" class=\"data row13 col8\" >-0.44</td>\n",
              "      <td id=\"T_87765_row13_col9\" class=\"data row13 col9\" >-0.44</td>\n",
              "      <td id=\"T_87765_row13_col10\" class=\"data row13 col10\" >0.35</td>\n",
              "      <td id=\"T_87765_row13_col11\" class=\"data row13 col11\" >-0.38</td>\n",
              "      <td id=\"T_87765_row13_col12\" class=\"data row13 col12\" >-0.34</td>\n",
              "      <td id=\"T_87765_row13_col13\" class=\"data row13 col13\" >1.00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {},
          "execution_count": 131
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w_df = df[['age', 'sex','trestbps', 'fbs', 'restecg', 'thalach', 'oldpeak', 'slope','target']]"
      ],
      "metadata": {
        "id": "_3s9edj-aHcs"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "X_train, X_test, y_train, y_test = train_test_split(w_df.iloc[:, :-1],\n",
        "                                                    w_df.iloc[:, -1],\n",
        "                                                    test_size=0.4)"
      ],
      "metadata": {
        "id": "IAOa96E3SJZ0"
      },
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sc = StandardScaler()\n",
        "# X_scaled = sc.fit_transform(X_train)\n",
        "# X_scaled = pd.DataFrame(data = X_scaled, columns = X.columns)"
      ],
      "metadata": {
        "id": "V7xKH3WG_K1R"
      },
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "std_slc = StandardScaler()\n",
        "pca = decomposition.PCA()\n",
        "lasso = linear_model.Lasso(alpha=-0.027407081977257608, normalize=False)\n",
        "pipe = Pipeline(steps=[(\"std_slc\", std_slc),\n",
        "                        (\"pca\", pca),\n",
        "                        (\"lasso\", lasso)])\n",
        "n_components = list(range(1,X_train.shape[1]+1,1))\n",
        "normalize = [True, False]\n",
        "selection = [\"cyclic\", \"random\"]\n",
        "parameters = dict(pca__n_components=n_components,\n",
        "                  lasso__normalize=normalize,\n",
        "                  lasso__selection=selection)\n",
        "\n"
      ],
      "metadata": {
        "id": "l2avV_PhVarC"
      },
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf = GridSearchCV(pipe, parameters)\n",
        "clf.fit(X_train, y_train)\n",
        "print(\"Best Number Of Components:\", clf.best_estimator_.get_params()[\"pca__n_components\"])\n",
        "CV_Result = cross_val_score(clf, X_train, y_train, cv=10, n_jobs=-1, scoring=\"r2\")\n",
        "print(CV_Result)\n",
        "print(CV_Result.mean())\n",
        "print(CV_Result.std())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "linJMu6eEWEn",
        "outputId": "6e477622-619f-450f-8071-13a3e3070b61"
      },
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.835e+01, tolerance: 1.229e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.305e+01, tolerance: 1.229e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.662e+01, tolerance: 1.229e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.865e+01, tolerance: 1.229e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.643e+01, tolerance: 1.230e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.345e+01, tolerance: 1.229e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.627e+01, tolerance: 1.229e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.835e+01, tolerance: 1.229e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.305e+01, tolerance: 1.229e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.662e+01, tolerance: 1.229e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.865e+01, tolerance: 1.229e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.643e+01, tolerance: 1.230e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.345e+01, tolerance: 1.229e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.627e+01, tolerance: 1.229e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.949e+02, tolerance: 1.229e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.881e+02, tolerance: 1.229e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.898e+02, tolerance: 1.229e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.907e+02, tolerance: 1.229e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.898e+02, tolerance: 1.230e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.812e+02, tolerance: 1.229e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.832e+02, tolerance: 1.229e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.865e+02, tolerance: 1.229e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.789e+02, tolerance: 1.229e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.796e+02, tolerance: 1.230e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.734e+02, tolerance: 1.229e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.752e+02, tolerance: 1.229e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.724e+02, tolerance: 1.229e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.726e+02, tolerance: 1.229e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.756e+02, tolerance: 1.230e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.602e+02, tolerance: 1.229e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.643e+02, tolerance: 1.229e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.680e+02, tolerance: 1.229e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.633e+02, tolerance: 1.229e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.693e+02, tolerance: 1.230e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.594e+02, tolerance: 1.229e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.612e+02, tolerance: 1.229e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.643e+02, tolerance: 1.229e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.631e+02, tolerance: 1.229e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.688e+02, tolerance: 1.230e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.571e+02, tolerance: 1.229e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.612e+02, tolerance: 1.229e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.596e+02, tolerance: 1.229e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.618e+02, tolerance: 1.229e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.601e+02, tolerance: 1.230e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.554e+02, tolerance: 1.229e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.587e+02, tolerance: 1.229e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.584e+02, tolerance: 1.229e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.612e+02, tolerance: 1.229e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.595e+02, tolerance: 1.230e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.508e+02, tolerance: 1.229e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.514e+02, tolerance: 1.229e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.522e+02, tolerance: 1.229e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.557e+02, tolerance: 1.229e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.529e+02, tolerance: 1.230e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.949e+02, tolerance: 1.229e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.881e+02, tolerance: 1.229e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.898e+02, tolerance: 1.229e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.907e+02, tolerance: 1.229e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.898e+02, tolerance: 1.230e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.812e+02, tolerance: 1.229e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.832e+02, tolerance: 1.229e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.865e+02, tolerance: 1.229e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.789e+02, tolerance: 1.229e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.796e+02, tolerance: 1.230e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.734e+02, tolerance: 1.229e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.752e+02, tolerance: 1.229e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.724e+02, tolerance: 1.229e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.726e+02, tolerance: 1.229e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.756e+02, tolerance: 1.230e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.602e+02, tolerance: 1.229e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.643e+02, tolerance: 1.229e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.680e+02, tolerance: 1.229e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.633e+02, tolerance: 1.229e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.693e+02, tolerance: 1.230e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.594e+02, tolerance: 1.229e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.612e+02, tolerance: 1.229e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.643e+02, tolerance: 1.229e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.631e+02, tolerance: 1.229e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.688e+02, tolerance: 1.230e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.571e+02, tolerance: 1.229e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.612e+02, tolerance: 1.229e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.596e+02, tolerance: 1.229e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.618e+02, tolerance: 1.229e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.601e+02, tolerance: 1.230e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.554e+02, tolerance: 1.229e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.587e+02, tolerance: 1.229e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.584e+02, tolerance: 1.229e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.612e+02, tolerance: 1.229e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.595e+02, tolerance: 1.230e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.508e+02, tolerance: 1.229e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.514e+02, tolerance: 1.229e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.522e+02, tolerance: 1.229e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.557e+02, tolerance: 1.229e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.529e+02, tolerance: 1.230e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.040e+02, tolerance: 1.537e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  model = cd_fast.enet_coordinate_descent(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Number Of Components: 5\n",
            "[0.16735441 0.13192966 0.26457894 0.28218503 0.31803073 0.16530921\n",
            " 0.24573889 0.19811619 0.31063277 0.1436611 ]\n",
            "0.22275369179771148\n",
            "0.06640090341916376\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(clf.best_estimator_.get_params()[\"lasso\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l997ooEmExgF",
        "outputId": "c48e2591-dc7c-4059-eda8-170d959101e4"
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lasso(alpha=-0.027407081977257608, normalize=False, selection='random')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import linear_model\n",
        "reg = linear_model.Lasso(alpha=0.1)\n",
        "logistic_preds = reg.fit(X_train, y_train).predict(X_test)\n",
        "logistic_acc = accuracy_score(y_test, logistic_preds.round().astype(int))\n",
        "print('Logistic Accuracy: {}'.format(logistic_acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RW33EodkSJcS",
        "outputId": "eb168f9e-dca8-4f9d-eede-155a7e56e126"
      },
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Accuracy: 0.7731707317073171\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df['target'].value_counts(normalize = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CoNT8zu-SJhd",
        "outputId": "0d4b4e10-8033-431a-9a8c-0bbd595442e9"
      },
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    0.513171\n",
              "0    0.486829\n",
              "Name: target, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 139
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report # metricas de validao\n",
        "print(classification_report(y_test, logistic_preds.round().astype(int)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U85PmovgEn-k",
        "outputId": "9789c4ac-8a04-437e-aafe-977ff0a43773"
      },
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.76      0.73      0.74       186\n",
            "           1       0.78      0.81      0.80       224\n",
            "\n",
            "    accuracy                           0.77       410\n",
            "   macro avg       0.77      0.77      0.77       410\n",
            "weighted avg       0.77      0.77      0.77       410\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy\n",
        "from sklearn import metrics\n",
        "\n",
        "confusion_matrix = metrics.confusion_matrix(y_test, logistic_preds.round(0).astype(int))\n",
        "\n",
        "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [False, True])\n",
        "\n",
        "cm_display.plot()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "id": "V82_gezNGSU1",
        "outputId": "2eca0397-d6a1-4ed7-86a2-a7b49f1402d8"
      },
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Calculate the confusion matrix\n",
        "#\n",
        "conf_matrix = confusion_matrix(y_test, logistic_preds.round(0).astype(int),normalize='true')\n",
        "#\n",
        "# Print the confusion matrix using Matplotlib\n",
        "#\n",
        "fig, ax = plt.subplots(figsize=(5, 5))\n",
        "ax.matshow(conf_matrix, cmap=plt.cm.Oranges, alpha=0.3)\n",
        "for i in range(conf_matrix.shape[0]):\n",
        "    for j in range(conf_matrix.shape[1]):\n",
        "        ax.text(x=j, y=i,s=str(round(conf_matrix[i, j]*100,1))+'%', va='center', ha='center', size='xx-large')\n",
        " \n",
        "plt.xlabel('Predictions', fontsize=18)\n",
        "plt.ylabel('Actuals', fontsize=18)\n",
        "plt.title('Lasso Regression - Filtered', fontsize=18)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        },
        "id": "C_Kdfd92Tycl",
        "outputId": "76fcece9-bf43-469f-90e5-9b4e49ac6015"
      },
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}